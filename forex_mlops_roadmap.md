# üéØ SYST√àME DE PR√âDICTION USD/EUR - ROADMAP APPRENTISSAGE

> **Niveau d'exigence** : Senior Data Engineer + ML Systems Engineer
> **Dur√©e estim√©e** : 6-9 mois (3-4h/jour)
> **Pr√©requis** : Python, SQL interm√©diaire, notions finance √©l√©mentaires

---

## ‚ö†Ô∏è CRITIQUE DU FRAMEWORK INITIAL

### Pourquoi le roadmap e-commerce ne marche PAS pour le Forex

| Aspect | E-commerce | Forex (Critique) |
|---|---|---|
| **Look-ahead bias** | Moins critique | FATAL - tu entra√Ænes avec l'avenir |
| **Validation temporelle** | K-fold OK | Invalide - rompt l'ordre chronologique |
| **Stationnarit√©** | Non pertinent | OBLIGATOIRE - USD/EUR n'est pas stationnaire |
| **Co√ªt d'erreur** | Notification inutile | Perte financi√®re r√©elle |
| **P√©riodicit√©** | Batch quotidien OK | Continu 24h/24 - gestion des gaps critique |
| **Drift monitoring** | Optionnel | Obligatoire - le march√© change 24/24 |

**Le pi√®ge conceptuel** : E-commerce traite les donn√©es comme statiques. Forex traite le temps comme dimension critique.

---

# üöÄ ARCHITECTURE SYST√àME

```
SOURCES TEMPS R√âEL (OHLCV, News, Volatilit√©)
            ‚Üì
INGESTION & VALIDATION DONN√âES
            ‚Üì
FEATURE ENGINEERING (sans look-ahead)
            ‚Üì
STATIONNARIT√â & TRANSFORMATION
            ‚Üì
TRAIN/VALIDATION/TEST (walk-forward UNIQUEMENT)
            ‚Üì
ENTRA√éNEMENT MOD√àLES
            ‚Üì
BACKTEST R√âALISTE (slippage, spread, commissions)
            ‚Üì
SERVING & D√âCISIONS
            ‚Üì
MONITORING FINANCIER (P&L vs backtest, drift)
```

---

# üìö ROADMAP D√âTAILL√âE (8 PHASES)

## üîπ PHASE 1 : FONDATIONS DATA ENGINEERING

### üéØ Objectif
Construire une pipeline d'ingestion **fiable, v√©rifiable et continue**.

### Pourquoi c'est diff√©rent de l'e-commerce ?
- EURUSD fonctionne 24h/24, 5j/5 ‚Üí gestion des gaps de temps
- Un gap de 4h en donn√©es = ton backtest devient mensonger
- Synchronisation temporelle = fondation de tout

### Concepts √† ma√Ætriser

**1. Time-Series Databases vs Bases relationnelles**
- Pourquoi PostgreSQL classique n'est pas optimis√© pour les s√©ries temporelles
- TimescaleDB/ClickHouse : compromis entre OLTP et OLAP
- Recherche : "Time-series database vs relational" et cas d'usage
- Question cl√© : Quel est le co√ªt en performance de stocker 5 ans de donn√©es horaires ?

**2. Fuseaux horaires et synchronisation**
- Notion de UTC comme r√©f√©rence unique
- Probl√®me r√©el : EURUSD n'est pas trad√© entre 20h et 22h UTC vendredi
- Recherche : "timezone handling in financial data" et "market hours"
- Question cl√© : Comment repr√©sentes-tu les gaps de week-end dans une base temps-s√©rie ?

**3. Sources de donn√©es pour Forex**
- Gratuit : yfinance, Alpha Vantage (limit√©)
- Professionnel : OANDA API, Interactive Brokers, FIX protocol
- Diff√©rences : latency, fiabilit√©, granularit√© (1min vs 5min vs 1h)
- Recherche : "Forex data API comparison" et documentations officielles
- Question cl√© : Pourquoi les donn√©es gratuit et pro donnent-elles parfois des r√©sultats l√©g√®rement diff√©rents ?

**4. Validation de donn√©es financi√®res**
- Coh√©rence OHLCV : High >= max(Open, Close), Low <= min(Open, Close)
- D√©tection de gaps anormaux
- D√©tection d'outliers (un prix qui saute de 20% en 1 heure = anomalie)
- Recherche : "OHLCV data validation rules" et "data quality for trading"
- Question cl√© : Qu'est-ce qu'une augmentation de volatilit√© "normale" vs une donn√©e corrompue ?

**5. Idempotence et reproductibilit√©**
- Capacit√© √† r√©ing√©rer des donn√©es sans cr√©er de doublons
- Versioning des donn√©es brutes
- Tracking de la source d'ingestion (quel broker, quelle API version)
- Recherche : "idempotent data pipelines" et "data lineage"
- Question cl√© : Si tu r√©ing√®res le m√™me jour 10 fois, que devrais-tu avoir en base ?

**6. Agr√©gations temporelles**
- 1h ‚Üí 4h ‚Üí 1D : comment construire les OHLC √† partir de timeframes plus petits
- Pr√©-calcul vs calcul √† la demande
- Recherche : "OHLC aggregation" et "candle consolidation"
- Question cl√© : Si tu as des donn√©es tick-level, comment construis-tu un chandelier 4h sans look-ahead ?

### Ressources √† consulter

- **Concepts** : Documentation TimescaleDB sur hypertables
- **Validation** : Great Expectations framework (concepts, pas code)
- **API** : Documentation officielle OANDA/IB sur data feeds
- **Best practices** : Articles sur "data contracts" en finance

### Livrables attendus

- Documentation de ta strat√©gie d'ingestion (sources choisies, fr√©quence, validation)
- Sch√©ma de base de donn√©es d√©taill√© avec justifications
- Analyse des gaps de donn√©es que tu trouves
- Plan de backfill historique (comment r√©cup√©rer 5 ans)

### Comp√©tences √† valider

‚úÖ Comprendre les diff√©rences entre PostgreSQL standard et time-series DB
‚úÖ Savoir pourquoi UTC + market hours = critique
‚úÖ Pouvoir √©num√©rer 10+ r√®gles de validation OHLCV
‚úÖ Concevoir idempotence dans un contexte temps-r√©el

---

## üîπ PHASE 2 : PIPELINES ROBUSTES & SCHEDULING

### üéØ Objectif
Z√©ro intervention manuelle. Les donn√©es s'ing√®rent continuellement et fiablement.

### Pourquoi diff√©rent de l'e-commerce ?

| E-commerce | Forex |
|---|---|
| Batch quotidien acceptable | Besoin continu (gap max 4h) |
| D√©pendances m√©tier | D√©pendances **temporelles strictes** |
| Retry simple en cas d'erreur | Retry d√©pend de l'heure du march√© |
| Donn√©es statiques apr√®s batch | Donn√©es "vivantes" durant les heures de march√© |

### Concepts √† ma√Ætriser

**1. DAG et d√©pendances temporelles**
- Concept de DAG (Directed Acyclic Graph)
- Diff√©rence entre d√©pendance de t√¢che vs d√©pendance temporelle
- Cron vs event-driven pour scheduling
- Recherche : "Airflow/Prefect DAG design" et "temporal dependencies"
- Question cl√© : Si une t√¢che s'ex√©cute √† 14h mais le march√© ferme √† 17h, comment tu g√®res les donn√©es de 14h-17h ?

**2. Idempotence absolue en production**
- Qu'est-ce qui emp√™che les doublons lors d'une r√©ex√©cution
- Upsert vs insert
- State management dans une pipeline
- Recherche : "idempotent operations in data pipelines"
- Question cl√© : Si une t√¢che fail, tu la rejoues. Comment s'assurer qu'elle ne cr√©e pas de dupliquatas ?

**3. Backfill massif**
- Ingestion r√©trospective de 5+ ans de donn√©es
- Gestion des rate limits (API ne permet que X requ√™tes/sec)
- Parall√©lisation vs s√©quentialit√©
- Recherche : "bulk data import strategies" et "rate-limited API calls"
- Question cl√© : Combien de temps devrait prendre l'ingestion de 60 ans de 1h candles EURUSD ?

**4. Gestion des erreurs et retry**
- Quand retry automatique a du sens
- Quand alerte humaine est n√©cessaire
- Exponential backoff
- Recherche : "retry strategies in data pipelines" et "circuit breaker pattern"
- Question cl√© : Si OANDA est down, tu retry jusqu'√† quand ? Combien de fois ?

**5. Monitoring et alertes**
- Qu'est-ce qu'une "t√¢che qui a l'air OK mais donne des donn√©es mauvaises"
- Monitoring du gap de donn√©es (derni√®re donn√©e de 4h il y a combien)
- M√©triques critiques √† exposer
- Recherche : "data pipeline monitoring" et "SLA for trading systems"
- Question cl√© : Tu dois d√©cider : "les donn√©es sont trop vieilles, j'arr√™te le trading". √Ä partir de quel seuil ?

### Ressources √† consulter

- **Orchestration** : Documentation Airflow ou Prefect (concepts d'ex√©cution, pas tutorial)
- **Design patterns** : Articles sur patterns de backfill et retry
- **Monitoring** : Articles sur "observability in data pipelines"

### Livrables attendus

- Diagramme DAG de ta pipeline avec d√©pendances explicites
- Strat√©gie de retry et alertes document√©e
- Plan d'ingestion historique avec estimations de temps
- M√©triques √† monitorer : liste + seuils

### Comp√©tences √† valider

‚úÖ Pouvoir dessiner et expliquer ta DAG
‚úÖ Citer 5 sc√©narios d'erreur et ta r√©action √† chaque
‚úÖ Expliquer pourquoi le retry exponentiel > retry lin√©aire
‚úÖ D√©finir quand tu arr√™tes le trading (data trop vieille)

---

## üîπ PHASE 3 : DATA QUALITY & CONTRACTS

### üéØ Objectif
D√©tecter les anomalies AVANT qu'elles ne cassent ton mod√®le et ton P&L.

### Pourquoi CRITIQUE en trading

**Sc√©nario r√©el probl√©matique** :
- Une source retourne tous les prix identiques pendant 2h
- Tu entra√Ænes un mod√®le avec ces 2h de bruit pur
- Ton mod√®le apprend un pattern invalide
- Pertes financi√®res en production

### Concepts √† ma√Ætriser

**1. Great Expectations framework**
- Notion d'expectation (assertion sur les donn√©es)
- Data quality suite
- Validation checkpoint
- Recherche : "Great Expectations tutorial" et "data contract concept"
- Question cl√© : Pourquoi v√©rifier "close > open" est plus fort que "pas de NULL" ?

**2. Validations OHLCV sp√©cifiques**
- High >= max(Open, Close) : toujours vrai?
- Low <= min(Open, Close) : toujours vrai?
- Volume > 0 : une bonne r√®gle?
- Close range [X, Y] : comment set les bornes?
- Recherche : "OHLC consistency validation" et "outlier detection in price data"
- Question cl√© : EUR/USD peut-il monter de plus de 2% en une heure?

**3. D√©tection d'anomalies temporelles**
- Gaps de temps : pas d'entr√©e pendant X temps = anomalie?
- Patterns impossibles : d√©tection
- Microstructure anormale (ex: prix montant strictement durant 50 min d'affil√©e)
- Recherche : "time-series anomaly detection" et "market data anomaly"
- Question cl√© : Comment distingues-tu une vraie volatilit√© d'une donn√©e corrompue?

**4. Stationnarit√© des donn√©es**
- Concept de s√©rie stationnaire vs non-stationnaire
- Pourquoi c'est important : si une s√©rie d√©rive, entra√Æner un mod√®le dessus = overfitting
- Tests : ADF, KPSS
- Solutions : diff√©renciation, log-returns
- Recherche : "stationary vs non-stationary time series" et "augmented dickey fuller test"
- Question cl√© : EUR/USD est-il stationnaire? Et si tu prends les returns (changements %) plut√¥t que le prix?

**5. Data contracts**
- Accord formel : "les donn√©es auront cette forme"
- Versioning des contrats (si contrat change = probl√®me)
- Sch√©ma √©volutif
- Recherche : "data contracts in data engineering" et "schema evolution"
- Question cl√© : Si OANDA change le format de l'API demain, tu fais quoi?

**6. Validation custom pour s√©ries temporelles**
- Look-ahead bias : impossibilit√© que t[n] d√©pende de t[n+1]
- Autocorr√©lation : si X[t] d√©pend de X[t-1], c'est normal ou artifact?
- Distribution : la distribution des prix est-elle stable (ou change-t-elle = drift)?
- Recherche : "look-ahead bias detection" et "autocorrelation in time series"
- Question cl√© : Comment v√©rifie-tu qu'une feature n'utilise PAS d'information future?

### Ressources √† consulter

- **Validation** : Great Expectations documentation (concepts)
- **Statistiques** : Cours sur stationnarit√© et tests ADF/KPSS
- **Finance** : Litt√©rature sur "market data quality"
- **Anomaly detection** : Articles sur detection en time-series

### Livrables attendus

- Suite compl√®te de 15+ assertions de validation
- Documentation de chaque assertion (pourquoi elle existe)
- Plan de test de stationnarit√© pour tes donn√©es
- D√©finition de data contract pour USD/EUR OHLCV

### Comp√©tences √† valider

‚úÖ Citer 10 v√©rifications d'OHLCV et leur logique
‚úÖ Expliquer stationnarit√© sans math (intuitivement)
‚úÖ Pouvoir identifier look-ahead bias dans une feature
‚úÖ Faire tourner et interpr√©ter un test ADF

---

## üîπ PHASE 4 : VERSIONING & LINEAGE

### üéØ Objectif
**Reproductibilit√© scientifique** : relancer l'exp√©rience 6 mois plus tard = r√©sultats identiques.

### Pourquoi c'est junior-level d'ignorer √ßa

**Sc√©nario r√©el** :
- Mod√®le X fait 55% d'accuracy
- 3 mois plus tard, tu veux le r√©entra√Æner
- Tu ne sais plus QUELLE version des donn√©es il a utilis√©
- R√©sultats sont diff√©rents
- Impossible de debugger : donn√©es change, code a chang√©, param√®tres hyper ont chang√© = tu ne sais pas quoi accuser

### Concepts √† ma√Ætriser

**1. Versioning des datasets**
- Notion de dataset immutable
- Hash/checksum de dataset (SHA256)
- Versioning via Git LFS ou DVC
- M√©tadonn√©es : qui a cr√©√©, quand, avec quelle source
- Recherche : "dataset versioning" et "DVC (Data Version Control)"
- Question cl√© : Si tu regeneres les features avec 1% des donn√©es en plus, tu incr√©mente la version comment?

**2. Reproducibility vs Repeatability**
- Reproductibilit√© : ex√©cuter EXACTEMENT la m√™me chose = m√™me r√©sultat
- Repeatabilit√© : r√©ex√©cuter = r√©sultats stables
- Seed al√©atoire, versions de libs, ordre d'ex√©cution
- Recherche : "reproducibility in ML" et "random seed importance"
- Question cl√© : Tu relances ton entra√Ænement, tu obtiens 55.01% vs 55.00% avant. C'est grave?

**3. Lineage des donn√©es**
- Tracking : d'o√π viennent les donn√©es (source ‚Üí transformation ‚Üí feature)
- Criticit√© : savoir qu'une feature d√©pend de telle transformation de telle source
- Recherche : "data lineage" et "data provenance"
- Question cl√© : Si tu d√©couvres un bug dans ta feature de RSI, comment tu identifies tous les mod√®les impact√©s?

**4. Feature versioning**
- Feature comme entit√© immuable
- Si tu changes une formule de feature = nouvelle version
- Tracking des features utilis√©es par quel mod√®le
- Recherche : "feature store" et "feature versioning"
- Question cl√© : Comment g√©rer quand tu d√©couvres un bug dans le calcul d'une feature utilis√©e par 5 mod√®les?

**5. Model checkpoints vs production**
- Sauvegarde √† chaque √©tape du training
- Capacit√© de rollback si nouveau mod√®le est pire
- M√©tadonn√©es attach√©es : score, date, donn√©es utilis√©es
- Recherche : "model versioning" et "model registry"
- Question cl√© : Tu d√©ploies un mod√®le, il fait -10% de P&L en 1 jour. Tu reverses √† quel point?

**6. Configurations immuables**
- Hyperparam√®tres version-controlled
- Seeds al√©atoires fix√©s
- Versions des libs freez√©es
- Recherche : "configuration management ML" et "dependency pinning"
- Question cl√© : Tu upgrades XGBoost de 1.5 √† 1.6, r√©sultats changent l√©g√®rement. Tu reverses ou acceptes?

### Ressources √† consulter

- **Versioning** : DVC documentation (concepts)
- **Lineage** : Apache Atlas, Marquez (concepts)
- **Reproducibility** : Litt√©rature sur reproduciblit√© en ML/Science
- **MLOps** : MLflow Model Registry documentation

### Livrables attendus

- Plan complet de versioning (datasets, features, mod√®les, configs)
- Structure Git pour tracer les changements
- Sch√©ma de lineage (donn√©es ‚Üí features ‚Üí mod√®les)
- Proc√©dure de rollback document√©e

### Comp√©tences √† valider

‚úÖ Expliquer diff√©rence reproducibilit√© vs repeatabilit√©
‚úÖ Faire un schema d'impact si une feature change
‚úÖ Pouvoir reverser un mod√®le (identifier quelle version)
‚úÖ Tracked changement entre 2 versions d'un mod√®le

---

## üîπ PHASE 5 : FEATURE ENGINEERING (Pont critique)

### üéØ Objectif
Transformer donn√©es brutes en signaux pertinents pour pr√©dire EURUSD.

### Pourquoi c'est le c≈ìur de la strat√©gie

**R√©alit√©** : 90% de la valeur pr√©dictive vient des features, pas du mod√®le. Un mod√®le basique sur bonnes features > mod√®le complexe sur mauvaises features.

### Concepts √† ma√Ætriser

**1. Principes de feature engineering en time-series**
- Feature = fonction d√©terministe des donn√©es
- Pas de look-ahead (crit√®re absolu)
- Domaine-specific > data-mining
- Causality : la feature doit avoir une relation causale avec le target
- Recherche : "feature engineering for time series" et "look-ahead bias"
- Question cl√© : "Close price de demain" est-elle une bonne feature pour pr√©dire "Direction demain"?

**2. Indicateurs techniques financiers**
- RSI (Relative Strength Index) : momentum
- MACD : trend
- Bollinger Bands : volatilit√©
- ATR (Average True Range) : volatilit√©
- SMA/EMA : trend
- Recherche : "technical indicators explanation" et "RSI calculation"
- Question cl√© : RSI > 70 = surachet√©. √Ä quoi √ßa correspond vraiment statistiquement?

**3. Statistical features**
- Volatility (√©cart-type des returns)
- Autocorrelation
- Skewness, Kurtosis
- Return distribution
- Recherche : "statistical features for price prediction" et "volatility estimation"
- Question cl√© : Si volatilit√© = 2%, √ßa te dit quoi sur les chances que prix monte demain?

**4. Market microstructure**
- Bid-ask spread (indication de liquidit√©)
- Volume profile
- Time-weighted average price
- Order flow imbalance
- Recherche : "market microstructure" et "FIX protocol basics"
- Question cl√© : Un volume √©lev√© avec prix stable = haussier ou baissier?

**5. Lagged features et rolling windows**
- Price[t-1], [t-2], [t-3]
- 20-period rolling average
- 5-period rolling volatility
- Attention : chaque lag = perte de donn√©e
- Recherche : "lag features in time series" et "window function"
- Question cl√© : Combien de lags tu dois garder avant que l'information soit trop vieille?

**6. Temporal features**
- Hour, day of week, month, quarter
- Is_market_open, Is_before_FOMC
- Days_since_economic_event
- Recherche : "cyclical features" et "temporal factors in trading"
- Question cl√© : EURUSD se comporte-t-il diff√©remment lundi matin vs jeudi apr√®s-midi?

**7. Cross-asset features**
- Corr√©lation avec S&P500, Or, P√©trole
- Forex indices (DXY = Dollar Index)
- Interest rates spread (US vs Eurozone)
- Recherche : "forex correlations" et "carry trade concept"
- Question cl√© : Si la FED rel√®ve les taux + le S&P monte, EUR/USD baisse. C'est universel?

**8. Absence de leakage**
- Information ne doit pas venir du future
- D√©pendances circulaires
- Forward-looking vs backward-looking
- Recherche : "data leakage in ML" et "target leakage detection"
- Question cl√© : Tu utilises "volatility realized demain". C'est du leakage?

**9. Feature scaling**
- Normalization (0-1) vs Standardization (mean=0, std=1)
- Impact sur diff√©rents mod√®les
- Recherche : "feature scaling for ML" et "when to normalize"
- Question cl√© : Tree-based model (XGBoost) a besoin de scaling? Et neural networks?

**10. Feature selection**
- Collin√©arit√© : si 2 features sont corr√©l√©es √† 0.95, garde-tu les 2?
- Importance (via mod√®le)
- Domain knowledge vs data-driven
- Recherche : "feature selection methods" et "multicollinearity"
- Question cl√© : Tu as 100 features. Tu les gardes toutes ou tu purges les moins importantes?

### Ressources √† consulter

- **Tech indicators** : Litt√©rature sur indicateurs techniques (Wikipedia, Trading books)
- **Statistics** : Cours sur statistiques temps-s√©rie
- **Feature design** : Articles sur feature engineering en ML
- **Finance** : Litt√©rature sur "what moves forex"

### Livrables attendus

- Catalogue complet des features envisag√©es (50+)
- Pour chaque feature : logique, calcul (sans code), justification
- Analyse de corr√©lations (quelles features sont redondantes?)
- Plan de d√©tection de look-ahead bias

### Comp√©tences √† valider

‚úÖ Citer 30 features pertinentes pour EURUSD
‚úÖ Expliquer pourquoi chaque feature a un pouvoir pr√©dictif
‚úÖ Identifier look-ahead bias dans 5 features suspectes
‚úÖ Justifier absence de leakage dans ta feature set

---

## üîπ PHASE 6 : ML TRA√áABLE

### üéØ Objectif
Transformer du ML (notebooks) en ing√©nierie reproductible et trac√©e.

### Pourquoi diff√©rent d'un notebook Kaggle

| Notebook | Production |
|---|---|
| "J'ai 88% d'accuracy" | "Quel mod√®le, quelle version, quel r√©sultat?" |
| Trial and error | Exp√©riences trac√©es syst√©matiquement |
| Pas de checkpoints | Checkpoints √† chaque √©tape |
| Pas de historique | Comparaison de 50 runs diff√©rents |
| Pas de d√©pendances | Requirements.txt + version freez√© |

### Concepts √† ma√Ætriser

**1. Baseline modeling**
- Mod√®le na√Øf : "demain = aujourd'hui"
- Mod√®le statistique : ARIMA
- Importance : savoir si ton mod√®le complexe > baseline
- Recherche : "baseline models for forecasting" et "persistence model"
- Question cl√© : Si tu pr√©dis "USD/EUR monte" 100% du temps, t'as quel accuracy?

**2. Training/Validation/Test split en time-series**
- Walk-forward : jamais future dans train
- Pourquoi K-fold invalide pour time-series
- Gap entre train et test (pr√©vient look-ahead)
- Recherche : "time series cross validation" et "walk forward validation"
- Question cl√© : Comment splits-tu 5 ans de donn√©es en train/val/test?

**3. M√©trique d'√©valuation appropri√©e**
- Accuracy ‚â† utile (peut √™tre trompeur)
- Precision/Recall (pour classification binaire)
- RMSE/MAE (pour r√©gression)
- Recherche : "evaluation metrics for trading" et "why accuracy is misleading"
- Question cl√© : Pr√©dire la direction (haut/bas) vs magnitude du mouvement = m√©triques diff√©rentes?

**4. Tuning d'hyperparam√®tres**
- Grid search vs Random search
- Validation set pour tuning
- Risque d'overfitting aux hyperparams
- Recherche : "hyperparameter tuning" et "random search vs grid search"
- Question cl√© : Tu tune sur train/val, tu testes sur test. Test est-il vraiment "unseen"?

**5. Multiple models vs Single model**
- Ensemble : combiner plusieurs mod√®les
- Stacking, Bagging, Boosting
- Quand ensemble aide, quand √ßa fait du mal
- Recherche : "ensemble methods in ML" et "when to use ensemble"
- Question cl√© : 5 mod√®les pr√©disant en moyenne > 1 bon mod√®le?

**6. Tracking avec MLflow**
- Concepts : experiments, runs, parameters, metrics, artifacts
- Logging automatique
- Comparaison de runs
- Recherche : "MLflow concepts" et "experiment tracking importance"
- Question cl√© : Comment retrouver "le mod√®le qui faisait 55% accuracy" 6 mois apr√®s?

**7. Model registry et versioning**
- Production-ready vs development
- Metadata : score, date, √©quipe
- Transition entre versions
- Recherche : "model registry" et "model lifecycle"
- Question cl√© : Tu d√©ploies version 5, elle fail. Tu reverses √† version 4. C'est facile?

**8. Model interpretability**
- Feature importance : quelles features font la d√©cision
- SHAP, LIME pour explainability
- Pourquoi c'est important en trading (tracabilit√© d'une perte)
- Recherche : "model interpretability" et "SHAP values"
- Question cl√© : Si mod√®le pr√©dit "EUR monte" bas√© sur 1 feature seule, c'est sus?

**9. Failing gracefully**
- Quand PAS faire de pr√©diction (confiance insuffisante)
- Threshold de confiance
- Fallback mechanisms
- Recherche : "prediction confidence" et "confidence threshold"
- Question cl√© : Si ton mod√®le est 51% s√ªr EUR monte, tu trades?

### Ressources √† consulter

- **Mod√®les** : Litt√©rature sur XGBoost, LightGBM, ARIMA
- **Validation** : Articles sur walk-forward validation
- **Tracking** : MLflow documentation (concepts)
- **Metrics** : Articles sur m√©triques en trading

### Livrables attendus

- Baseline model document√© (na√Øf + ARIMA)
- Plan de train/val/test walk-forward
- 3-5 mod√®les diff√©rents essay√©s (r√©sum√© r√©sultats)
- Dashboard MLflow avec 20+ runs compar√©s
- Feature importance analysis

### Comp√©tences √† valider

‚úÖ Expliquer pourquoi K-fold ne marche pas
‚úÖ Impl√©menter walk-forward validation
‚úÖ Comparer 3 mod√®les objectivement
‚úÖ Interpr√©ter feature importance d'un mod√®le

---

## üîπ PHASE 7 : SERVING & D√âPLOIEMENT

### üéØ Objectif
Rendre le mod√®le consommable en temps r√©el par des d√©cisions de trading.

### Pourquoi c'est critique

**Diff√©rence** :
- Notebook : "G√©n√®re pr√©diction pour donn√©es fixes, une fois"
- Production : "Re√ßoit nouveau datapoint toutes les heures, g√©n√®re signal en < 100ms, g√®re erreurs"

### Concepts √† ma√Ætriser

**1. Pr√©diction en production vs training**
- Distribution de donn√©es peut changer (drift)
- Latency requirements (100ms max)
- Batch vs online predictions
- Recherche : "prediction serving" et "online vs batch prediction"
- Question cl√© : Tu re√ßois nuevo price √† 14:00:00.000. √Ä 14:00:00.100, tu veux une d√©cision. Possible?

**2. API REST fundamentals**
- Endpoint = URL qui re√ßoit donn√©es, retourne pr√©diction
- Request/Response format
- Error handling
- Recherche : "REST API basics" et "API design for ML"
- Question cl√© : Si l'API r√©pond "error 500", ton robot de trading fait quoi?

**3. Framework FastAPI vs Flask**
- Diff√©rences en performance
- Async/await pour latency
- Validation de donn√©es entrantes
- Recherche : "FastAPI vs Flask" et "async Python for APIs"
- Question cl√© : FastAPI c'est 10x plus vite que Flask. √áa vaut la peine d'apprendre?

**4. Model serving patterns**
- Single model : une pr√©diction = un mod√®le
- Ensemble : combiner pr√©dictions de plusieurs mod√®les
- Canary deployment : nouveaux mod√®les sur petit % du traffic
- Recherche : "model serving patterns" et "canary deployment"
- Question cl√© : Tu as mod√®le V5 et V6. Comment tu testes V6 sur 10% du traffic?

**5. Caching et performance**
- Requ√™tes identiques = m√™me r√©ponse (sans recalcul)
- Cache invalidation (quand le cache devient p√©rim√©)
- Recherche : "caching in APIs" et "cache invalidation"
- Question cl√© : Si tu mets en cache "EUR/USD close = 1.095", tu mets le cache jusqu'√† quand?

**6. Monitoring de l'API**
- Latency : temps de r√©ponse
- Throughput : requ√™tes/seconde
- Error rate : % d'erreurs
- Recherche : "API monitoring" et "SLA metrics"
- Question cl√© : Si API r√©pond lentement (500ms au lieu de 100ms), c'est une alerte?

**7. Rollback et versioning**
- Capacit√© de reverser rapidement si buggu
- Blue-green deployment : 2 versions en parall√®le
- Recherche : "blue-green deployment" et "zero-downtime deployment"
- Question cl√© : Si nouveau mod√®le tue le P&L, tu le desactive en combien de temps?

**8. Docker et containerization**
- Isolement : chaque version du mod√®le dans son container
- Reproductibilit√© : m√™me image = m√™me r√©sultats
- Recherche : "Docker for ML" et "containerization concepts"
- Question cl√© : Docker c'est quoi exactement et pourquoi √ßa aide?

**9. Feedback loop et r√©collecte de donn√©es**
- Chaque pr√©diction = donn√©es d'apprentissage potentielles
- Tracking de "pr√©diction vs r√©alit√©"
- Collecte de ces √©carts pour retraining
- Recherche : "feedback loop" et "active learning"
- Question cl√© : Une pr√©diction "EUR monte" quand r√©alit√© "EUR baisse". Tu l'utilises pour retraining?

### Ressources √† consulter

- **API** : Documentation FastAPI (concepts, architecture)
- **Serving** : Articles sur model serving patterns
- **Deployment** : Articles sur blue-green, canary
- **Docker** : Docker documentation (concepts)

### Livrables attendus

- Sp√©cification API (endpoint, payload, response)
- Architecture de serving (sch√©ma)
- Plan de monitoring (quoi monitorer, seuils d'alerte)
- Proc√©dure de rollback document√©e
- Docker strategy (si applicable)

### Comp√©tences √† valider

‚úÖ Dessiner une API REST pour pr√©dictions
‚úÖ Expliquer probl√®me de latency vs accuracy
‚úÖ Concevoir plan de rollback < 5 minutes
‚úÖ Monitorer 5 m√©triques critiques

---

## üîπ PHASE 8 : MONITORING & DRIFT

### üéØ Objectif
Savoir QUAND ton mod√®le ment, AVANT qu'il ne co√ªte cher.

### Pourquoi c'est diff√©rent de "normal monitoring"

| Syst√®me normal | Trading |
|---|---|
| API lente = bad UX | API lente = trade manqu√© = perte |
| Donn√©es manquantes = "retry" | Donn√©es manquantes = d√©cision sur donn√©es vieilles = perte |
| Mod√®le drift = r√©sultats moins bons | Mod√®le drift = pertes financi√®res = CRITIQUE |

### Concepts √† ma√Ætriser

**1. Data drift (Feature drift)**
- Distribution des features change
- Exemple : volatilit√© moyenne passe de 2% √† 5%
- D√©tection : comparer distribution actuelle vs historique
- Recherche : "data drift detection" et "statistical tests for drift"
- Question cl√© : Si volatilit√© monte de 2% √† 2.5%, c'est un drift significatif?

**2. Prediction drift**
- Distribution des pr√©dictions change
- Exemple : mod√®le pr√©dit "EUR monte" 70% du temps vs 40% avant
- Indication possibilit√© que donn√©es ont chang√©
- Recherche : "prediction drift" et "monitoring predictions"
- Question cl√© : Quelle % de "EUR monte" vs "EUR baisse" est normal?

**3. Target drift / Performance drift**
- R√©alit√© change
- Exemple : mod√®le pr√©disait 55% accuracy, maintenant 48%
- Mettre en evidence le plus grave
- Recherche : "target drift" et "model performance monitoring"
- Question cl√© : √Ä quelle baisse d'accuracy tu d√©cides "le mod√®le ne marche plus"?

**4. M√©thodologie d'alerting**
- Seuils : "accuracy < 50%" ou "data drift p-value < 0.01"
- Sensibilit√© vs Sp√©cificit√©
- False alarms vs missed problems
- Recherche : "alerting strategies" et "false positive rate"
- Question cl√© : Tu veux une alerte √† chaque 0.1% de changement (trop) ou une fois par semaine (trop tard)?

**5. Tools pour monitoring**
- Evidently : monitoring drift
- Great Expectations : validation continue
- Custom dashboards
- Recherche : "Evidently AI" et "production monitoring tools"
- Question cl√© : Quels sont les outils gratuit vs payant pour monitoring?

**6. Retraining trigger**
- √Ä quel moment tu r√©entra√Ænes
- Automatiqu vs manuel
- Risque : r√©entra√Æner sur donn√©es buggu√©es = pire
- Recherche : "model retraining strategies" et "drift detection"
- Question cl√© : Tous les jours? Une fois par semaine? Ou √† la demande?

**7. A/B testing**
- Tester nouveau mod√®le sur subset du traffic
- Mesurer impact sur P&L
- Rollout progressif
- Recherche : "A/B testing in production" et "online experiments"
- Question cl√© : Tu as mod√®le V5 vs V6. Comment tu compares lequel est meilleur?

**8. Model shadows et canary**
- Lancer nouveau mod√®le en parallel, comparer sans utiliser ses pr√©dictions
- Risque z√©ro de d√©ploiement mauvais mod√®le
- Recherche : "shadow models" et "canary deployment"
- Question cl√© : Comment tu testes un mod√®le avant de lui donner 100% du traffic?

**9. Incident response**
- Processus : d√©tecter anomalie ‚Üí alerter ‚Üí investiguer ‚Üí fixer
- Communication : qui doit √™tre notifi√©
- Rollback : reverser √† quoi?
- Recherche : "incident response procedures" et "on-call playbooks"
- Question cl√© : Une alerte drift √† 3am. Qui se l√®ve? Quel est le plan?

**10. P&L tracking et backtesting**
- Backtest du mod√®le actuel vs mod√®le ancien
- Tracking : "ce mod√®le a perdu 500‚Ç¨ ce mois"
- Decision : reverser ou laisser?
- Recherche : "live trading P&L tracking" et "performance attribution"
- Question cl√© : Comment tu sais si baisse de P&L = bad model ou bad market?

### Ressources √† consulter

- **Drift detection** : Litt√©rature sur tests statistiques (KS test, chi-square)
- **Tools** : Evidently, Great Expectations documentation
- **Monitoring** : Articles sur "observability in ML"
- **Finance** : Articles sur "model governance in trading"

### Livrables attendus

- Dashboard de monitoring (7+ m√©triques)
- D√©finition de seuils d'alerte (avec justifications)
- Proc√©dure d'incident response
- Plan de retraining automatique
- Strat√©gie de A/B testing pour nouveaux mod√®les

### Comp√©tences √† valider

‚úÖ D√©tecter 5 types de drift diff√©rents
‚úÖ Faire un dashboard de monitoring (m√™me simple)
‚úÖ Concevoir une proc√©dure d'alerte
‚úÖ Interpr√©ter un r√©sultat "P&L baisse : drift ou march√©?"

---

# üéì COMP√âTENCES FINALES ACQUISES

| Domaine | Niveau | Distinction |
|---|---|---|
| **Data Engineering** | Avanc√© | Gestion du temps comme dimension critique |
| **SQL** | Expert | Time-series optimization |
| **Data Quality** | Rare | D√©tection d'anomalies financi√®res |
| **Pipelines** | Solide | Idempotence, backfill, monitoring |
| **Feature Engineering** | Expert | No look-ahead, causalit√© |
| **Time-series ML** | Avanc√© | Walk-forward validation, stationnarit√© |
| **MLOps** | Professionnel | Versioning, reproducibility, monitoring |
| **Serving** | Op√©rationnel | Latency, rollback, feedback loops |
| **Risk & Trading** | Professionnel | P&L tracking, incident response |

---

# üö´ CE QUE TU N'APPRENDRAS PAS

‚ùå Kubernetes (complexe trop t√¥t)
‚ùå LLMs (hors scope trading)
‚ùå High-frequency trading (microseconde latency)
‚ùå Algorithmes complexes type deep RL
‚ùå Options pricing / Derivatives (finance avanc√©e)

---

# üìä RESSOURCES CRITIQUES PAR PHASE

## Phase 1
- TimescaleDB official docs
- OANDA API documentation  
- "Time Series Databases" article/white-papers
- Great Expectations concepts

## Phase 2
- Airflow/Prefect documentation (DAG concepts)
- "Data Pipeline Patterns"
- Monitoring tools comparison

## Phase 3
- Great Expectations framework
- ADF/KPSS test tutorials
- "Data Quality for ML"
- "Look-ahead bias in practice"

## Phase 4
- DVC conceptual overview
- MLflow documentation
- "Reproducibility in ML" papers
- Data lineage tools

## Phase 5
- "Technical Analysis Explained"
- Time-series feature engineering guides
- "Microstructure of Financial Markets"
- Forex correlation studies

## Phase 6
- XGBoost/LightGBM documentation
- "Time Series Forecasting" textbooks
- Walk-forward validation tutorials
- MLflow tracking concepts

## Phase 7
- FastAPI documentation
- REST API design principles
- "Model Serving at Scale"
- Docker concepts

## Phase 8
- Evidently documentation
- "Monitoring ML Systems"
- Statistical drift tests
- Trading incident response papers

---

# üéØ PHRASE CL√â

> **"En trading, tu √©choues rarement √† cause des mod√®les.
> Tu √©choues √† cause des donn√©es mal g√©r√©es, du timing, et du drift non d√©tect√©."**

Ma√Ætriser ces 8 phases = Senior Data Engineer + ML Systems Engineer. Pr√™t?